# -*- coding: utf-8 -*-
"""guestures.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1Tk-w1BxUuVbDY-jRl5qC0YFXV8mCx6
"""

"""!rm -r guestures

!unzip guestures"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from sklearn.cluster import DBSCAN, MiniBatchKMeans, KMeans
from sklearn.metrics.pairwise import pairwise_distances_argmin
import os
import seaborn as sb
import random

RANDOM_SEED = 22340

data = None
for dir, _, files in os.walk("guestures/"):
  files = sorted(files)
  
  for file in files:
    tmp_data = np.genfromtxt(dir + file, delimiter=',')
    label = np.zeros((tmp_data.shape[0])) + int(file.split('.')[0])
    tmp_data = np.column_stack((tmp_data, label)).astype(np.int16) 
    if data is None:
      data = tmp_data
    else:
      print(data.shape, tmp_data.shape)
      data = np.row_stack((data, tmp_data))

"""visualize sample data"""

idx = 12
fig = plt.figure()
fig.suptitle('53 samples of a guesture ', fontsize=20)
plt.xlabel('sample number', fontsize=18)
plt.ylabel('analog readings ', fontsize=16)

for i in range(5):
  x = data[idx*53:idx*53+53, i]
  plt.plot(x)
plt.show()

data_shuffle = data.copy()
np.random.shuffle(data_shuffle)

def kmeans_clustering(X, **kmean_kwargs):
   
    kmeans = KMeans(random_state=RANDOM_SEED, **kmean_kwargs)
    kmeans.fit(X)
    cluster_centers = np.sort(kmeans.cluster_centers_, axis = 0)
    # labels = pairwise_distances_argmin(X,cluster_centers)
    labels = kmeans.labels_
    
    
    return kmeans, labels

def plot_wcss_graph(dataset, k_range, **kmean_kwargs):
   
    wcss = []
    for k in k_range:
        kmeans, _ = kmeans_clustering(dataset, n_clusters = k)
        wcss.append(kmeans.inertia_)

    fig = plt.figure(figsize=(15,15))
    plt.xticks(k_range)
    plt.plot(k_range, wcss)
  
    fig.suptitle('Plot of WCSS with K value', fontsize=20)
    plt.xlabel('Number of clusters', fontsize=18)
    plt.ylabel('WCSS (inertia)', fontsize=16)
    plt.plot()

plot_wcss_graph(data_shuffle[:,:6], range(10, 50))

clf, labels = kmeans_clustering(data_shuffle[:,:6], n_clusters = 29)

label_n_cluster = np.zeros((40, 40)).astype(np.int)
for i,cluster in enumerate(labels):
 label_n_cluster[data_shuffle[i, 5]-1, cluster-1] = label_n_cluster[data_shuffle[i, 5]-1, cluster-1] + 1

fig = plt.figure(figsize=(12,12))
plt.matshow(label_n_cluster, fignum =1)

selected_guestures = []
for i in range(label_n_cluster.shape[1]): # through cols
  print('labels in cluster:', i, end='|  ')
  selected = None
  max = -1
  for j in range(label_n_cluster.shape[0]): # through rows
    if label_n_cluster[j, i] != 0:
      print(j, end =" ")
      if label_n_cluster[j, i] > max:
        max = label_n_cluster[j, i]
        selected = j

  if selected is not None:
    selected_guestures.append(selected)
  print(" ")

len(selected_guestures)

selected_guestures

filtered_dataset = np.zeros((len(selected_guestures)*53, 6))
for i, gues in enumerate(selected_guestures):
  filtered_dataset[i*53:i*53+53, :] = data[gues*53:gues*53+53, :]

import pandas as pd

df = pd.DataFrame(filtered_dataset)
cor = df.corr()
  
# plotting correlation heatmap
dataplot = sb.heatmap(cor, cmap="YlGnBu", annot=True)
  
# displaying heatmap
plt.show()

"""Classification


"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report
from sklearn.metrics import confusion_matrix
from sklearn import preprocessing
import pickle

def fit_tree_classifier(X, y, **decisiontree_kwargs):
  clf = DecisionTreeClassifier(random_state=RANDOM_SEED)
  clf.fit(X,y)
  return clf


def fit_knn_classifier(X, y, **knn_kwargs):
  clf = KNeighborsClassifier()
  clf.fit(X,y)
  return clf

def fit_mlp_classifier(X,y, **mlp_kwargs):
  clf = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1,**mlp_kwargs)
  clf.fit(X,y)
  return clf


def evaluate_classification(y_true, y_predicted):
    cm = confusion_matrix(y_true, y_predicted)
    report = classification_report(y_true, y_predicted)
    print(report)

    fig, ax = plt.subplots(figsize=(15,15))
    disp = ConfusionMatrixDisplay(cm, display_labels = None)
    
    disp.plot(ax=ax)
    plt.show()

train_data, test_data = train_test_split(filtered_dataset, test_size=0.25, random_state=RANDOM_SEED)
X_train = train_data[:,:5]
Y_train = train_data[:,5]

X_test = test_data[:,:5]
Y_test = test_data[:,5]

tree = fit_tree_classifier(X_train, Y_train)
knn = fit_knn_classifier(X_train, Y_train)

tree_pred = tree.predict(X_test)
knn_pred = knn.predict(X_test)

evaluate_classification(Y_test, tree_pred)

evaluate_classification(Y_test, knn_pred)

scaler = preprocessing.StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

mlp = fit_mlp_classifier(X_train_scaled, Y_train,  hidden_layer_sizes = (10,))
mlp_pred = mlp.predict(X_test_scaled)

evaluate_classification(Y_test, mlp_pred)



"""Save the Model

"""

knnPickle = open('knnpickle_file', 'wb') 
pickle.dump(knn, knnPickle)  
knnPickle.close()

"""Load and predict"""

loaded_model = pickle.load(open('knnpickle_file', 'rb'))
result = loaded_model.predict(X_test)

evaluate_classification(Y_test, result)

import time


X = np.array([808,	282,	645,	802,	822	]).reshape(1,5)
t1 = time.time()
result = loaded_model.predict(X) 
t2 = time.time()
print(result, (t2-t1)*1000)

